{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to vx (Python 3.9.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import tifffile\n",
    "import stackview\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b5bcf691e545d4ab2e2e9ecf188fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(ImageWidget(height=256, width=256),)),)), IntSlider(value=16, des…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load and visualize an image stack and convert to dtype uInt8\n",
    "\n",
    "image_path = \"20170928CH_Exp8_M28.mt2.tif\" # e.g. stacked OME-Tiff \n",
    "image = tifffile.imread(image_path)\n",
    "image_uint8 = (image / image.max() * 255).astype(np.uint8)\n",
    "\n",
    "image = torch.Tensor(image_uint8)\n",
    "\n",
    "mean = 127.5\n",
    "std = 127.5\n",
    "\n",
    "stackview.slice(image, continuous_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop_z(image: torch.Tensor):\n",
    "    # Utility to center-crop an image stack along the z-dimension\n",
    "    shape_z = image.shape[0]\n",
    "    start_z = shape_z // 2 - 15\n",
    "    end_z = shape_z // 2 + 15\n",
    "    return image[start_z:end_z, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945299b1efbe4d6e951d31d8d5a08c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(ImageWidget(height=196, width=152),)),)), IntSlider(value=15, des…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize and reshape/crop the data for input into the model\n",
    "\n",
    "# Expected input shape for network: 196, 152, 30\n",
    "image_transforms = transforms.Compose([transforms.Normalize(mean, std), transforms.CenterCrop(size=(196, 152)), transforms.Lambda(lambda image: center_crop_z(image))])\n",
    "image_normalized = image_transforms(image)\n",
    "\n",
    "stackview.slice(image_normalized, continuous_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and run inference\n",
    "model = torch.jit.load(\"lesion_model.pt\")\n",
    "\n",
    "batch = image_normalized.permute(2, 1, 0)[None, None, :]\n",
    "logits = model(batch)\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "probabilities = softmax(logits[\"segment_type_task\"][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold probabilities and create masks\n",
    "\n",
    "BACKGROUND_LABEL = 0\n",
    "LESION_LABEL = 1\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "probabilities_numpy = probabilities.detach().numpy()\n",
    "\n",
    "lesion_probabilities = probabilities_numpy[LESION_LABEL,:]\n",
    "background_probabilities = probabilities_numpy[BACKGROUND_LABEL,:]\n",
    "\n",
    "lesion_mask = np.where(lesion_probabilities > THRESHOLD, 1, 0)\n",
    "background_mask = np.where(background_probabilities > THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bd2f993af94476a91226205f3d9fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(ImageWidget(height=392, width=304),)),)), HBox(children=(Button(d…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the results\n",
    "\n",
    "lesion_image = np.pad(lesion_mask, ((21, 21), (21, 21), (1,1))).transpose(2, 1, 0)\n",
    "background_image = np.pad(background_mask, ((21, 21), (21, 21), (1,1))).transpose(2, 1, 0)\n",
    "\n",
    "stackview.switch({\"raw+lesion\": 0.7 * image_normalized + 0.3 * lesion_image, \"raw\": image_normalized, \"lesion\": lesion_image, \"background\": background_image}, zoom_factor=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
